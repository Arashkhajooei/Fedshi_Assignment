{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ─── 1 │ Imports & configuration (hardened) ─────────────────────────────────────\n",
    "\n",
    "import asyncio, json, os, math\n",
    "from typing import List, Dict\n",
    "\n",
    "import aiohttp, backoff, chromadb, nest_asyncio, openai, pandas as pd\n",
    "from chromadb.config import Settings\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "nest_asyncio.apply()            # let Jupyter nest event loops\n",
    "\n",
    "# ── Config knobs ───────────────────────────────────────────────────────────────\n",
    "API_BATCH_SIZE        = 8        # ↓ to keep prompt+JSON < 8 k tokens\n",
    "CONCURRENT_LIMIT      = 100      # simultaneous OpenAI calls\n",
    "WINDOW_SIZE_BATCHES   = 2_000    # batches kept in memory at once\n",
    "MODEL_NAME            = \"gpt-4.1-nano-2025-04-14\"\n",
    "TIMEOUT_SECONDS       = 60\n",
    "MAX_RETRIES           = 10\n",
    "\n",
    "CSV_PATH              = \"data/Books.csv\"\n",
    "OUTPUT_CSV            = \"Enriched_Books_Metadata.csv\"\n",
    "CHROMA_DIR            = \"chroma_books\"\n",
    "CHROMA_COLLECTION     = \"books_metadata\"\n",
    "\n",
    "# ── OpenAI client & retry helpers ──────────────────────────────────────────────\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "RETRYABLE_EXCEPTIONS = (\n",
    "    openai.RateLimitError,\n",
    "    openai.APIConnectionError,\n",
    "    openai.APITimeoutError,\n",
    "    aiohttp.ClientError,\n",
    "    asyncio.TimeoutError,\n",
    ")\n",
    "\n",
    "def _log_backoff(details):\n",
    "    \"\"\"Pretty back‑off logger that won’t KeyError.\"\"\"\n",
    "    session_id = details[\"args\"][1]\n",
    "    wait       = details[\"wait\"]\n",
    "    tries      = details[\"tries\"]\n",
    "    max_tries  = details.get(\"max_tries\", MAX_RETRIES)\n",
    "    print(f\"🔁 Retry batch {session_id}: sleeping {wait:.1f}s \"\n",
    "          f\"(attempt {tries}/{max_tries})\")\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    RETRYABLE_EXCEPTIONS,\n",
    "    max_tries=MAX_RETRIES,\n",
    "    max_time=300,\n",
    "    jitter=backoff.full_jitter,\n",
    "    on_backoff=_log_backoff,\n",
    ")\n",
    "async def call_openai_with_timeout(book_batch: List[Dict], session_id: int):\n",
    "    \"\"\"\n",
    "    Send one batch to OpenAI with both soft & hard timeouts, plus JSON\n",
    "    validation so bad payloads go through the retry loop.\n",
    "    \"\"\"\n",
    "    # Build user message\n",
    "    user_lines = [\n",
    "        f\"id: {b['row_id']}, title: {b['Book_Title']}, author: {b['Book_Author']}\"\n",
    "        for b in book_batch\n",
    "    ]\n",
    "    input_text = \"\\n\".join(user_lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful librarian assistant. For each book provided \"\n",
    "        \"generate a JSON object with keys: summary, genre, category, theme, \"\n",
    "        \"tone, audience, and the SAME id you received. Return the list under \"\n",
    "        \"the top‑level key 'data'. Example: {\\\"data\\\": [{\\\"id\\\": 123, ...}]}\"\n",
    "    )\n",
    "\n",
    "    response = await asyncio.wait_for(\n",
    "        client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": input_text},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.2,\n",
    "            max_tokens=2048,\n",
    "            timeout=TIMEOUT_SECONDS - 5,\n",
    "        ),\n",
    "        timeout=TIMEOUT_SECONDS,\n",
    "    )\n",
    "\n",
    "    # Validate JSON – invalid → raise so back‑off retries\n",
    "    try:\n",
    "        content = json.loads(response.choices[0].message.content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise openai.APIError(f\"Bad JSON: {e}\") from e\n",
    "\n",
    "    return content.get(\"data\", [])\n",
    "\n",
    "async def process_batch(book_batch: List[Dict], session_id: int, sem: asyncio.Semaphore):\n",
    "    \"\"\"Concurrency‑guarded wrapper with alignment + fault placeholders.\"\"\"\n",
    "    async with sem:\n",
    "        try:\n",
    "            raw_result = await call_openai_with_timeout(book_batch, session_id)\n",
    "        except Exception as exc:            # survived all retries\n",
    "            print(f\"❌ Batch {session_id} final failure: {exc}\")\n",
    "            raw_result = []\n",
    "\n",
    "    # Safeguard: only dicts with an id\n",
    "    result_map = {\n",
    "        item[\"id\"]: item\n",
    "        for item in raw_result\n",
    "        if isinstance(item, dict) and \"id\" in item\n",
    "    }\n",
    "\n",
    "    aligned = [\n",
    "        result_map.get(\n",
    "            row[\"row_id\"],\n",
    "            {\"id\": row[\"row_id\"], \"genre\": \"Error\", \"theme\": \"Missing\", \"tone\": \"\"}\n",
    "        )\n",
    "        for row in book_batch\n",
    "    ]\n",
    "    return aligned\n",
    "\n",
    "# ─── 2 │ Main orchestration coroutine ─────────────────────────────────────────\n",
    "async def main():\n",
    "    print(\"📚 Loading dataset …\")\n",
    "    books_df = (\n",
    "        pd.read_csv(CSV_PATH, encoding=\"latin-1\", low_memory=False)\n",
    "          .loc[:, [\"ISBN\", \"Book-Title\", \"Book-Author\"]]\n",
    "          .dropna().drop_duplicates().reset_index(drop=True)\n",
    "          .rename(columns={\"Book-Title\": \"Book_Title\", \"Book-Author\": \"Book_Author\"})\n",
    "    )\n",
    "    # Add a unique row_id column that persists through batching\n",
    "    books_df[\"row_id\"] = books_df.index\n",
    "\n",
    "    batches = [\n",
    "        books_df.iloc[i : i + API_BATCH_SIZE].to_dict(\"records\")\n",
    "        for i in range(0, len(books_df), API_BATCH_SIZE)\n",
    "    ]\n",
    "    total_batches = len(batches)\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)\n",
    "\n",
    "    print(f\"🚀 Processing {len(books_df):,} books \"\n",
    "          f\"in {total_batches:,} batches (≤{CONCURRENT_LIMIT} concurrent)…\")\n",
    "\n",
    "    all_metadata: List[Dict] = []\n",
    "    failed_batches = 0\n",
    "\n",
    "    # Feed the loop in windows so we never hold 27 k tasks at once\n",
    "    for w_start in range(0, total_batches, WINDOW_SIZE_BATCHES):\n",
    "        w_end  = min(w_start + WINDOW_SIZE_BATCHES, total_batches)\n",
    "        window = [process_batch(batches[i], i, semaphore)\n",
    "                  for i in range(w_start, w_end)]\n",
    "\n",
    "        for fut in tqdm(asyncio.as_completed(window),\n",
    "                        total=len(window),\n",
    "                        desc=f\"Batches {w_start}–{w_end-1}\"):\n",
    "            result = await fut\n",
    "            if result and result[0].get(\"genre\") == \"Error\":\n",
    "                failed_batches += 1\n",
    "            all_metadata.extend(result)\n",
    "\n",
    "    print(f\"✅ OpenAI enrichment done. Failed batches: {failed_batches}\")\n",
    "\n",
    "    # ── Merge & save CSV ───────────────────────────────────────────────────────\n",
    "    metadata_df = pd.DataFrame(all_metadata).set_index(\"id\")\n",
    "    final_df = (\n",
    "        books_df.set_index(\"row_id\")\n",
    "        .join(metadata_df, how=\"left\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"💾 Saved enriched metadata → {OUTPUT_CSV}\")\n",
    "\n",
    "    # ── Persist to ChromaDB ───────────────────────────────────────────────────\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        persist_directory=CHROMA_DIR,\n",
    "        anonymized_telemetry=False\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(CHROMA_COLLECTION)\n",
    "\n",
    "    valid_rows = final_df.dropna(subset=[\"genre\"]).query(\"genre != 'Error'\")\n",
    "    if valid_rows.empty:\n",
    "        print(\"⚠️ No valid rows to add to ChromaDB.\")\n",
    "        return\n",
    "\n",
    "    collection.add(\n",
    "        ids=valid_rows[\"ISBN\"].tolist(),\n",
    "        documents=[\n",
    "            (f\"Title: {r.Book_Title} by {r.Book_Author}. \"\n",
    "             f\"Genre: {r.genre}. Theme: {r.theme}. Tone: {r.tone}.\")\n",
    "            for r in valid_rows.itertuples()\n",
    "        ],\n",
    "        metadatas=[\n",
    "            {\n",
    "                \"title\":   r.Book_Title,\n",
    "                \"author\":  r.Book_Author,\n",
    "                \"genre\":   r.genre,\n",
    "                \"theme\":   r.theme,\n",
    "                \"tone\":    r.tone,\n",
    "            }\n",
    "            for r in valid_rows.itertuples()\n",
    "        ],\n",
    "    )\n",
    "    print(f\"✅ Added {len(valid_rows):,} rows to ChromaDB.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ─── 3 │ Kick off the async workflow (⇧⏎) ──────────────────────────────────────\n",
    "\n",
    "# %%\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6222173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.khajooei\\AppData\\Local\\Temp\\ipykernel_24316\\3547328730.py:10: DtypeWarning: Columns (18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Enriched_Books_Metadata.csv\").head(100)\n",
      "Normalizing metadata: 100%|██████████| 1/1 [02:47<00:00, 167.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Normalized data saved to 'Normalized_Enriched_Metadata.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "OPENAI_API_KEY = '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'\n",
    "\n",
    "\n",
    "# Load enriched metadata\n",
    "df = pd.read_csv(\"Enriched_Books_Metadata.csv\").head(100)\n",
    "# Load your OpenAI API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "# Select and clean relevant columns\n",
    "df = df[['ISBN', 'Book_Title', 'Book_Author', 'summary', 'genre', 'category', 'theme', 'tone', 'audience']]\n",
    "df = df.dropna(subset=['summary', 'genre', 'category', 'theme', 'tone', 'audience'])\n",
    "\n",
    "# Prepare batch processing\n",
    "batch_size = 1000\n",
    "results = []\n",
    "\n",
    "# Normalization function using OpenAI\n",
    "def normalize_metadata_batch(batch):\n",
    "    prompt = (\n",
    "        \"You are a metadata normalization engine. Given a list of book metadata in JSON, return the same list with \"\n",
    "        \"each field ('genre', 'category', 'theme', 'tone', 'audience') standardized across books. Merge similar values \"\n",
    "        \"to unified tags for better recommendation use.\\n\\n\"\n",
    "        f\"Metadata:\\n{json.dumps(batch, ensure_ascii=False)}\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1-nano-2025-04-14\",\n",
    "            instructions=\"Return JSON list with normalized metadata.\",\n",
    "            input=prompt\n",
    "        )\n",
    "        return json.loads(response.output_text)\n",
    "    except Exception as e:\n",
    "        return [{\"error\": str(e)} for _ in batch]\n",
    "\n",
    "# Batch processing with progress\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Normalizing metadata\"):\n",
    "    batch = df.iloc[i:i + batch_size].to_dict(orient='records')\n",
    "    normalized = normalize_metadata_batch(batch)\n",
    "    for original, updated in zip(batch, normalized):\n",
    "        results.append({**original, **updated})\n",
    "    time.sleep(1)  # be kind to rate limits\n",
    "\n",
    "# Save result\n",
    "pd.DataFrame(results).to_csv(\"Normalized_Enriched_Metadata.csv\", index=False)\n",
    "print(\"✅ Normalized data saved to 'Normalized_Enriched_Metadata.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c882955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Clustering column: genre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batches: 100%|██████████| 271/271 [05:07<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Clustering column: category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batches: 100%|██████████| 271/271 [04:53<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Clustering column: theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batches: 100%|██████████| 271/271 [06:52<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Clustering column: tone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batches: 100%|██████████| 271/271 [06:39<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Clustering column: audience\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batches: 100%|██████████| 271/271 [06:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All metadata columns clustered and saved to 'clustered_outputs/'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load and clean dataset\n",
    "df = pd.read_csv(\"Enriched_Books_Metadata.csv\", low_memory=False)\n",
    "df = df[['ISBN', 'Book_Title', 'Book_Author', 'summary', 'genre', 'category', 'theme', 'tone', 'audience']]\n",
    "df.dropna(subset=['genre', 'category', 'theme', 'tone', 'audience'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine all metadata columns\n",
    "text_columns = ['genre', 'category', 'theme', 'tone', 'audience']\n",
    "\n",
    "# Initialize model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create batches\n",
    "batch_size = 1000\n",
    "batches = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "# Track results\n",
    "clustered_data = []\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"clustered_outputs\", exist_ok=True)\n",
    "\n",
    "# Clustering settings per column\n",
    "n_clusters_dict = {\n",
    "    'genre': 20,\n",
    "    'category': 20,\n",
    "    'theme': 25,\n",
    "    'tone': 20,\n",
    "    'audience': 15,\n",
    "}\n",
    "\n",
    "# Process each metadata column\n",
    "for col in text_columns:\n",
    "    print(f\"\\n🔍 Clustering column: {col}\")\n",
    "    encoded_batches = []\n",
    "\n",
    "    # Encode texts\n",
    "    for batch in tqdm(batches, desc=\"Embedding Batches\"):\n",
    "        texts = batch[col].astype(str).tolist()\n",
    "        embeddings = model.encode(texts, show_progress_bar=False)\n",
    "        encoded_batches.append(embeddings)\n",
    "\n",
    "    # Stack all embeddings\n",
    "    all_embeddings = np.vstack(encoded_batches)\n",
    "\n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters_dict[col], random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(all_embeddings)\n",
    "\n",
    "    # Assign back to df\n",
    "    df[f\"{col}_cluster\"] = labels\n",
    "\n",
    "    # Save partial clustered outputs\n",
    "    df[['ISBN', col, f\"{col}_cluster\"]].to_csv(f\"clustered_outputs/clustered_{col}.csv\", index=False)\n",
    "\n",
    "# Save final full clustered metadata\n",
    "df.to_csv(\"clustered_outputs/clustered_books_metadata.csv\", index=False)\n",
    "print(\"\\n✅ All metadata columns clustered and saved to 'clustered_outputs/'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ccfb76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final normalized metadata saved as 'final_normalized_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load enriched book metadata\n",
    "df = pd.read_csv(\"Enriched_Books_Metadata.csv\", low_memory=False)\n",
    "\n",
    "# Load clustered data\n",
    "clustered_df = pd.read_csv(\"clustered_outputs/clustered_books_metadata.csv\")\n",
    "\n",
    "# Ensure matching ISBNs (safe merge)\n",
    "merged_df = df.merge(\n",
    "    clustered_df[['ISBN', 'genre_cluster', 'category_cluster', 'theme_cluster', 'tone_cluster', 'audience_cluster']],\n",
    "    on='ISBN',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Replace original metadata with cluster labels\n",
    "merged_df['genre'] = merged_df['genre_cluster']\n",
    "merged_df['category'] = merged_df['category_cluster']\n",
    "merged_df['theme'] = merged_df['theme_cluster']\n",
    "merged_df['tone'] = merged_df['tone_cluster']\n",
    "merged_df['audience'] = merged_df['audience_cluster']\n",
    "\n",
    "# Drop the old cluster columns\n",
    "merged_df.drop(columns=[\n",
    "    'summary','genre_cluster', 'category_cluster', 'theme_cluster', 'tone_cluster', 'audience_cluster'\n",
    "], inplace=True)\n",
    "\n",
    "# Save final normalized dataset\n",
    "merged_df.to_csv(\"final_normalized_data.csv\", index=False)\n",
    "print(\"✅ Final normalized metadata saved as 'final_normalized_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "410a119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Book_Author</th>\n",
       "      <th>genre</th>\n",
       "      <th>category</th>\n",
       "      <th>theme</th>\n",
       "      <th>tone</th>\n",
       "      <th>audience</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>Ab 12 J.</th>\n",
       "      <th>Kim Fupz Aakeson</th>\n",
       "      <th>A story about Ulla's adventures and everyday life, exploring friendship, family, and personal growth through relatable experiences for young readers.  : 1.3,</th>\n",
       "      <th>Juvenile Fiction / Coming-of-Age</th>\n",
       "      <th>Friendship / Family / Personal Growth</th>\n",
       "      <th>Light-hearted / Reflective</th>\n",
       "      <th>Young Readers / Pre-teens</th>\n",
       "      <th>67841</th>\n",
       "      <th>67843</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271353</th>\n",
       "      <td>0440400988</td>\n",
       "      <td>There's a Bat in Bunk Five</td>\n",
       "      <td>Paula Danziger</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271354</th>\n",
       "      <td>0525447644</td>\n",
       "      <td>From One to One Hundred</td>\n",
       "      <td>Teri Sloat</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271355</th>\n",
       "      <td>006008667X</td>\n",
       "      <td>Lily Dale : The True Story of the Town that Ta...</td>\n",
       "      <td>Christine Wicker</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271356</th>\n",
       "      <td>0192126040</td>\n",
       "      <td>Republic (World's Classics)</td>\n",
       "      <td>Plato</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271357</th>\n",
       "      <td>0767409752</td>\n",
       "      <td>A Guided Tour of Rene Descartes' Meditations o...</td>\n",
       "      <td>Christopher  Biffle</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271358 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                         Book_Title  \\\n",
       "0       0195153448                                Classical Mythology   \n",
       "1       0002005018                                       Clara Callan   \n",
       "2       0060973129                               Decision in Normandy   \n",
       "3       0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4       0393045218                             The Mummies of Urumchi   \n",
       "...            ...                                                ...   \n",
       "271353  0440400988                         There's a Bat in Bunk Five   \n",
       "271354  0525447644                            From One to One Hundred   \n",
       "271355  006008667X  Lily Dale : The True Story of the Town that Ta...   \n",
       "271356  0192126040                        Republic (World's Classics)   \n",
       "271357  0767409752  A Guided Tour of Rene Descartes' Meditations o...   \n",
       "\n",
       "                 Book_Author  genre  category  theme  tone  audience title  \\\n",
       "0         Mark P. O. Morford   14.0      14.0   23.0   1.0       8.0   NaN   \n",
       "1       Richard Bruce Wright   10.0       2.0    4.0  12.0       0.0   NaN   \n",
       "2               Carlo D'Este    6.0       0.0    3.0   8.0       3.0   NaN   \n",
       "3           Gina Bari Kolata   14.0      14.0    8.0   1.0       3.0   NaN   \n",
       "4            E. J. W. Barber   14.0      14.0   23.0   1.0       3.0   NaN   \n",
       "...                      ...    ...       ...    ...   ...       ...   ...   \n",
       "271353        Paula Danziger    2.0      18.0    7.0  10.0       2.0   NaN   \n",
       "271354            Teri Sloat   14.0      14.0   21.0   5.0       2.0   NaN   \n",
       "271355      Christine Wicker   14.0       3.0   16.0  17.0       4.0   NaN   \n",
       "271356                 Plato   16.0       5.0    2.0  19.0       8.0   NaN   \n",
       "271357   Christopher  Biffle   19.0       7.0   11.0   8.0       5.0   NaN   \n",
       "\n",
       "       author  Ab 12 J.  Kim Fupz Aakeson  \\\n",
       "0         NaN       NaN               NaN   \n",
       "1         NaN       NaN               NaN   \n",
       "2         NaN       NaN               NaN   \n",
       "3         NaN       NaN               NaN   \n",
       "4         NaN       NaN               NaN   \n",
       "...       ...       ...               ...   \n",
       "271353    NaN       NaN               NaN   \n",
       "271354    NaN       NaN               NaN   \n",
       "271355    NaN       NaN               NaN   \n",
       "271356    NaN       NaN               NaN   \n",
       "271357    NaN       NaN               NaN   \n",
       "\n",
       "        A story about Ulla's adventures and everyday life, exploring friendship, family, and personal growth through relatable experiences for young readers.  : 1.3,    \\\n",
       "0                                                     NaN                                                                                                                 \n",
       "1                                                     NaN                                                                                                                 \n",
       "2                                                     NaN                                                                                                                 \n",
       "3                                                     NaN                                                                                                                 \n",
       "4                                                     NaN                                                                                                                 \n",
       "...                                                   ...                                                                                                                 \n",
       "271353                                                NaN                                                                                                                 \n",
       "271354                                                NaN                                                                                                                 \n",
       "271355                                                NaN                                                                                                                 \n",
       "271356                                                NaN                                                                                                                 \n",
       "271357                                                NaN                                                                                                                 \n",
       "\n",
       "        Juvenile Fiction / Coming-of-Age  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "...                                  ...   \n",
       "271353                               NaN   \n",
       "271354                               NaN   \n",
       "271355                               NaN   \n",
       "271356                               NaN   \n",
       "271357                               NaN   \n",
       "\n",
       "        Friendship / Family / Personal Growth  Light-hearted / Reflective  \\\n",
       "0                                         NaN                         NaN   \n",
       "1                                         NaN                         NaN   \n",
       "2                                         NaN                         NaN   \n",
       "3                                         NaN                         NaN   \n",
       "4                                         NaN                         NaN   \n",
       "...                                       ...                         ...   \n",
       "271353                                    NaN                         NaN   \n",
       "271354                                    NaN                         NaN   \n",
       "271355                                    NaN                         NaN   \n",
       "271356                                    NaN                         NaN   \n",
       "271357                                    NaN                         NaN   \n",
       "\n",
       "        Young Readers / Pre-teens 67841 67843 json  \n",
       "0                             NaN   NaN   NaN  NaN  \n",
       "1                             NaN   NaN   NaN  NaN  \n",
       "2                             NaN   NaN   NaN  NaN  \n",
       "3                             NaN   NaN   NaN  NaN  \n",
       "4                             NaN   NaN   NaN  NaN  \n",
       "...                           ...   ...   ...  ...  \n",
       "271353                        NaN   NaN   NaN  NaN  \n",
       "271354                        NaN   NaN   NaN  NaN  \n",
       "271355                        NaN   NaN   NaN  NaN  \n",
       "271356                        NaN   NaN   NaN  NaN  \n",
       "271357                        NaN   NaN   NaN  NaN  \n",
       "\n",
       "[271358 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dd812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Generating names for genre clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 8469/8469 [04:48<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Generating names for category clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   6%|▌         | 506/8469 [01:00<19:41,  6.74it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"clustered_outputs/clustered_books_metadata.csv\", low_memory=False)\n",
    "original_df = pd.read_csv(\"Enriched_Books_Metadata.csv\", low_memory=False)\n",
    "\n",
    "# Clean columns\n",
    "columns = ['genre', 'category', 'theme', 'tone', 'audience']\n",
    "df.dropna(subset=columns, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Init model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Map clusters to labels\n",
    "cluster_name_maps = {}\n",
    "\n",
    "for col in columns:\n",
    "    print(f\"\\n🔍 Generating names for {col} clusters\")\n",
    "    texts = df[col].astype(str).tolist()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    # Cluster again to find representatives\n",
    "    n_clusters = df[f\"{col}_cluster\"].nunique()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Map cluster index to closest label\n",
    "    cluster_names = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = np.array(texts)[np.array(labels) == i]\n",
    "        cluster_names[i] = \", \".join(cluster_points[:2])  # use top 2 representative phrases\n",
    "    \n",
    "    cluster_name_maps[col] = cluster_names\n",
    "    df[f\"{col}_normalized\"] = df[f\"{col}_cluster\"].map(cluster_names)\n",
    "\n",
    "# Replace original columns in the enriched metadata\n",
    "original_df = original_df.merge(\n",
    "    df[['ISBN'] + [f\"{col}_normalized\" for col in columns]],\n",
    "    on=\"ISBN\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Replace old metadata columns with normalized ones\n",
    "for col in columns:\n",
    "    original_df[col] = original_df[f\"{col}_normalized\"]\n",
    "    original_df.drop(columns=[f\"{col}_normalized\"], inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "original_df.to_csv(\"final_normalized_metadata.csv\", index=False)\n",
    "print(\"\\n✅ Saved as 'final_normalized_metadata.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61669a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
